{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5 Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "856f0246a02c4e44a2b1b9860440f8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_855de523deda461cb41bbf5fb7743e12",
              "IPY_MODEL_189502a388634729ad5f28c642825b84",
              "IPY_MODEL_e4462eac82b04f15b199e8d6af9ba4d0"
            ],
            "layout": "IPY_MODEL_7da057902ff448789ae6af4bad5e2754"
          }
        },
        "855de523deda461cb41bbf5fb7743e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68adba266a94f8dab3e5cf0be2f8cb8",
            "placeholder": "​",
            "style": "IPY_MODEL_c846a0bffea44e2a8de67f0afa0c4d3f",
            "value": "100%"
          }
        },
        "189502a388634729ad5f28c642825b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e37d57eeb8d040069cc172c86a8c4354",
            "max": 818322941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1ef43e066594bc1826c3a8e513d8f59",
            "value": 818322941
          }
        },
        "e4462eac82b04f15b199e8d6af9ba4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560aa164d0e442b497d5b6a249ec6f2a",
            "placeholder": "​",
            "style": "IPY_MODEL_06820e75d75b4a26ab2cf30cd80b360e",
            "value": " 780M/780M [04:01&lt;00:00, 6.00MB/s]"
          }
        },
        "7da057902ff448789ae6af4bad5e2754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68adba266a94f8dab3e5cf0be2f8cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c846a0bffea44e2a8de67f0afa0c4d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e37d57eeb8d040069cc172c86a8c4354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1ef43e066594bc1826c3a8e513d8f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "560aa164d0e442b497d5b6a249ec6f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06820e75d75b4a26ab2cf30cd80b360e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay-jojo-cheng/deep-tsundoku/blob/master/notebooks/sample_prototyping_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "This example notebook shows how to use our project template to begin prototyping. You can copy this into the relevant notebook subfolder you will be working from, e.g. deep-tsundoku/notebooks/0_book_detection, as a starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone GitHub [repository](https://github.com/jay-jojo-cheng/deep-tsundoku), install [dependencies](https://github.com/jay-jojo-cheng/deep-tsundoku/requirements.txt) and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7538e22-30fe-46d7-bd18-502f48a41d94"
      },
      "source": [
        "!git clone https://github.com/jay-jojo-cheng/deep-tsundoku # clone\n",
        "%cd deep-tsundoku\n",
        "%pip install -qr requirements.txt # install requirements\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 61b247c Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 38.4/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "g14Hyix1kTS3",
        "outputId": "61092d07-3488-40d9-c3dc-c11444d6bc7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classify   hubconf.py  notebooks\t setup.cfg  tutorial.ipynb\n",
            "configs    LICENSE     pyproject.toml\t setup.py   utils\n",
            "data\t   logs        README.md\t src\t    val.py\n",
            "detect.py  Makefile    requirements.txt  tests\n",
            "export.py  models      scripts\t\t train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Detect\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "```shell\n",
        "python detect.py --source 0  # webcam\n",
        "                          img.jpg  # image \n",
        "                          vid.mp4  # video\n",
        "                          path/  # directory\n",
        "                          'path/*.jpg'  # glob\n",
        "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b516c9f-cc4b-455d-f7c5-8f76970bfc43"
      },
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 61b247c Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:03<00:00, 3.92MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "image 1/2 /content/deep-tsundoku/data/images/bus.jpg: 640x480 4 persons, 1 bus, 15.9ms\n",
            "image 2/2 /content/deep-tsundoku/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 13.2ms\n",
            "Speed: 0.5ms pre-process, 14.5ms inference, 20.5ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Validate\n",
        "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "856f0246a02c4e44a2b1b9860440f8bf",
            "855de523deda461cb41bbf5fb7743e12",
            "189502a388634729ad5f28c642825b84",
            "e4462eac82b04f15b199e8d6af9ba4d0",
            "7da057902ff448789ae6af4bad5e2754",
            "e68adba266a94f8dab3e5cf0be2f8cb8",
            "c846a0bffea44e2a8de67f0afa0c4d3f",
            "e37d57eeb8d040069cc172c86a8c4354",
            "f1ef43e066594bc1826c3a8e513d8f59",
            "560aa164d0e442b497d5b6a249ec6f2a",
            "06820e75d75b4a26ab2cf30cd80b360e"
          ]
        },
        "outputId": "54b18a42-eada-4fa3-95e3-42f891314790"
      },
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/780M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "856f0246a02c4e44a2b1b9860440f8bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85558bd4-5347-4073-9319-6f37ece359ed"
      },
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights yolov5s.pt --data coco.yaml --img 640 --half"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/deep-tsundoku/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 61b247c Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:02<00:00, 2059.61it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/val2017.cache\n",
            "                 Class     Images  Instances          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:07<00:00,  2.32it/s]\n",
            "                   all       5000      36335       0.67      0.521      0.566      0.371\n",
            "Speed: 0.1ms pre-process, 2.7ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)\n",
            "\n",
            "Evaluating pycocotools mAP... saving runs/val/exp/yolov5s_predictions.json...\n",
            "loading annotations into memory...\n",
            "Done (t=0.42s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=5.78s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=76.42s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=13.46s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.572\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.402\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.516\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.566\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.625\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.723\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "\n",
        "Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
        "\n",
        "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
        "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
        "- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n",
        "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
        "<br><br>\n",
        "\n",
        "A **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select YOLOv5 🚀 logger {run: 'auto'}\n",
        "logger = 'W&B' #@param ['TensorBoard', 'W&B']\n",
        "\n",
        "if logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir runs/train\n",
        "elif logger == 'W&B':\n",
        "  %pip install -q wandb\n",
        "  import wandb; wandb.login()"
      ],
      "metadata": {
        "id": "i3oKtE4g-aNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673ec716-7183-4e21-a464-ba454a0d5492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjay-jojo-cheng\u001b[0m (\u001b[33mcausalml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d79ac04-48a2-4bd8-d344-28fc31208f4e"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjay-jojo-cheng\u001b[0m (\u001b[33mcausalml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "warning: no common commits\n",
            "remote: Enumerating objects: 12285, done.\u001b[K\n",
            "Command 'git fetch ultralytics' timed out after 5 seconds\n",
            "Parse error at \"'andardiz'\": Expected stringEnd\n",
            "YOLOv5 🚀 4f6cd9d Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/deep-tsundoku/deep-tsundoku/wandb/run-20220913_203718-1x32lz2k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-mountain-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/causalml/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/causalml/YOLOv5/runs/1x32lz2k\u001b[0m\n",
            "\n",
            "Dataset not found ⚠️, missing paths ['/content/deep-tsundoku/datasets/coco128/images/train2017']\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to coco128.zip...\n",
            "100% 6.66M/6.66M [00:00<00:00, 14.9MB/s]\n",
            "Dataset download success ✅ (1.5s), saved to \u001b[1m/content/deep-tsundoku/datasets\u001b[0m\n",
            "YOLOv5 temporarily requires wandb version 0.12.10 or below. Some features may not work as expected.\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 220MB/s]\n",
            "\n",
            "Receiving objects:  46% (5652/12285), 4.55 MiB | 690.00 KiB/s   \n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "remote: Total 12285 (delta 0), reused 0 (delta 0), pack-reused 12285\u001b[K\n",
            "Receiving objects: 100% (12285/12285), 12.57 MiB | 1.07 MiB/s, done.\n",
            "Resolving deltas: 100% (8464/8464), done.\n",
            "From https://github.com/ultralytics/yolov5\n",
            " * [new branch]      add/weights_dir       -> ultralytics/add/weights_dir\n",
            " * [new branch]      cls/test_reset_params -> ultralytics/cls/test_reset_params\n",
            " * [new branch]      coco-segments         -> ultralytics/coco-segments\n",
            " * [new branch]      exp/copy-paste        -> ultralytics/exp/copy-paste\n",
            " * [new branch]      exp/sort_obj_iou      -> ultralytics/exp/sort_obj_iou\n",
            " * [new branch]      fix/rgb_albumentations -> ultralytics/fix/rgb_albumentations\n",
            " * [new branch]      ghost                 -> ultralytics/ghost\n",
            " * [new branch]      master                -> ultralytics/master\n",
            " * [new branch]      pip                   -> ultralytics/pip\n",
            " * [new branch]      sorted_iou_objectloss -> ultralytics/sorted_iou_objectloss\n",
            " * [new branch]      study_activations     -> ultralytics/study_activations\n",
            " * [new branch]      test/alive_progress   -> ultralytics/test/alive_progress\n",
            " * [new branch]      test/conv_reduction   -> ultralytics/test/conv_reduction\n",
            " * [new branch]      test/convtranspose    -> ultralytics/test/convtranspose\n",
            " * [new branch]      test/decouple         -> ultralytics/test/decouple\n",
            " * [new branch]      test/dw5              -> ultralytics/test/dw5\n",
            " * [new branch]      test/seeds            -> ultralytics/test/seeds\n",
            " * [new branch]      ultralytics/HUB       -> ultralytics/ultralytics/HUB\n",
            " * [new branch]      update/loss           -> ultralytics/update/loss\n",
            " * [new branch]      update/textlogger     -> ultralytics/update/textlogger\n",
            " * [new branch]      update/yaml_activation -> ultralytics/update/yaml_activation\n",
            " * [new branch]      v7.0                  -> ultralytics/v7.0\n",
            " * [new branch]      v7.0.1                -> ultralytics/v7.0.1\n",
            " * [new tag]         v1.0                  -> v1.0\n",
            " * [new tag]         v2.0                  -> v2.0\n",
            " * [new tag]         v3.0                  -> v3.0\n",
            " * [new tag]         v3.1                  -> v3.1\n",
            " * [new tag]         v4.0                  -> v4.0\n",
            " * [new tag]         v5.0                  -> v5.0\n",
            " * [new tag]         v6.0                  -> v6.0\n",
            " * [new tag]         v6.1                  -> v6.1\n",
            " * [new tag]         v6.2                  -> v6.2\n",
            "Transferred 349/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/deep-tsundoku/datasets/coco128/labels/train2017' images and labels...126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<00:00, 2145.06it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/deep-tsundoku/datasets/coco128/labels/train2017.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 128/128 [00:00<00:00, 239.66it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/deep-tsundoku/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 128/128 [00:01<00:00, 102.93it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/2      4.75G    0.04644    0.06704    0.01985        226        640: 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                 Class     Images  Instances          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:08<00:00,  2.20s/it]\n",
            "                   all        128        929      0.656      0.611      0.679      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/2       4.7G    0.04692    0.06578    0.01769        267        640: 100% 8/8 [00:02<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:02<00:00,  1.57it/s]\n",
            "                   all        128        929      0.742       0.62      0.721      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/2       4.7G    0.04378    0.05896    0.01652        181        640: 100% 8/8 [00:02<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:02<00:00,  1.48it/s]\n",
            "                   all        128        929      0.749      0.658      0.744      0.487\n",
            "\n",
            "3 epochs completed in 0.010 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.9MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.9MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "                 Class     Images  Instances          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:05<00:00,  1.36s/it]\n",
            "                   all        128        929      0.751      0.657      0.744      0.486\n",
            "                person        128        254      0.851      0.717      0.804      0.527\n",
            "               bicycle        128          6      0.765      0.551      0.688      0.419\n",
            "                   car        128         46      0.666      0.413      0.553      0.236\n",
            "            motorcycle        128          5      0.676          1      0.928      0.631\n",
            "              airplane        128          6          1      0.981      0.995      0.677\n",
            "                   bus        128          7      0.567      0.714      0.757      0.655\n",
            "                 train        128          3          1      0.576      0.913      0.523\n",
            "                 truck        128         12      0.584      0.333      0.493      0.262\n",
            "                  boat        128          6      0.846      0.333      0.476      0.187\n",
            "         traffic light        128         14      0.782      0.286      0.356      0.223\n",
            "             stop sign        128          2       0.81          1      0.995      0.796\n",
            "                 bench        128          9      0.694      0.444      0.651      0.306\n",
            "                  bird        128         16      0.971          1      0.995      0.621\n",
            "                   cat        128          4      0.786          1      0.995      0.797\n",
            "                   dog        128          9      0.975      0.667      0.895      0.637\n",
            "                 horse        128          2      0.837          1      0.995      0.597\n",
            "              elephant        128         17      0.925      0.882      0.937      0.695\n",
            "                  bear        128          1      0.671          1      0.995      0.995\n",
            "                 zebra        128          4      0.862          1      0.995      0.923\n",
            "               giraffe        128          9      0.639      0.788      0.874      0.683\n",
            "              backpack        128          6      0.803        0.5      0.779      0.313\n",
            "              umbrella        128         18      0.831       0.82      0.849      0.458\n",
            "               handbag        128         19      0.834      0.264      0.397      0.213\n",
            "                   tie        128          7       0.75      0.714      0.782      0.506\n",
            "              suitcase        128          4      0.626          1      0.995      0.547\n",
            "               frisbee        128          5      0.717        0.8       0.76      0.675\n",
            "                  skis        128          1      0.742          1      0.995      0.107\n",
            "             snowboard        128          7      0.725      0.756      0.818      0.526\n",
            "           sports ball        128          6       0.63      0.667      0.602      0.317\n",
            "                  kite        128         10      0.715      0.501      0.586      0.218\n",
            "          baseball bat        128          4      0.726        0.5      0.541        0.2\n",
            "        baseball glove        128          7      0.559      0.429      0.466      0.312\n",
            "            skateboard        128          5          1      0.587      0.711      0.498\n",
            "         tennis racket        128          7       0.75      0.429      0.531      0.295\n",
            "                bottle        128         18      0.526      0.389      0.531      0.258\n",
            "            wine glass        128         16      0.701      0.875      0.904      0.539\n",
            "                   cup        128         36      0.791      0.583      0.823      0.536\n",
            "                  fork        128          6          1      0.328      0.493      0.341\n",
            "                 knife        128         16      0.784      0.682      0.689      0.389\n",
            "                 spoon        128         22      0.783        0.5      0.639       0.37\n",
            "                  bowl        128         28      0.768      0.592       0.71      0.504\n",
            "                banana        128          1       0.78          1      0.995      0.298\n",
            "              sandwich        128          2          1          0      0.359      0.326\n",
            "                orange        128          4      0.768      0.842      0.945      0.632\n",
            "              broccoli        128         11      0.466      0.364      0.453      0.351\n",
            "                carrot        128         24      0.684      0.667       0.73      0.473\n",
            "               hot dog        128          2      0.305          1      0.995       0.92\n",
            "                 pizza        128          5      0.817          1      0.995       0.75\n",
            "                 donut        128         14      0.662          1      0.964      0.806\n",
            "                  cake        128          4      0.867          1      0.995      0.803\n",
            "                 chair        128         35       0.55      0.629      0.614      0.338\n",
            "                 couch        128          6      0.852      0.667      0.811      0.518\n",
            "          potted plant        128         14      0.747      0.786      0.829      0.463\n",
            "                   bed        128          3      0.589      0.333      0.753      0.416\n",
            "          dining table        128         13      0.824      0.364      0.579      0.343\n",
            "                toilet        128          2      0.915          1      0.995      0.796\n",
            "                    tv        128          2      0.555          1      0.995      0.746\n",
            "                laptop        128          3          1          0      0.586      0.302\n",
            "                 mouse        128          2          1          0      0.106     0.0531\n",
            "                remote        128          8          1      0.618      0.634      0.547\n",
            "            cell phone        128          8      0.659        0.5      0.455      0.246\n",
            "             microwave        128          3      0.759          1      0.995      0.756\n",
            "                  oven        128          5      0.317        0.4      0.431       0.29\n",
            "                  sink        128          6       0.44      0.333      0.363      0.233\n",
            "          refrigerator        128          5      0.544        0.8      0.798      0.568\n",
            "                  book        128         29      0.572      0.241      0.341       0.15\n",
            "                 clock        128          9       0.77      0.889      0.932      0.713\n",
            "                  vase        128          2      0.406          1      0.995      0.895\n",
            "              scissors        128          1          1          0      0.497     0.0554\n",
            "            teddy bear        128         21      0.871      0.646       0.82      0.534\n",
            "            toothbrush        128          5      0.906          1      0.995       0.66\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 ▁▆██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 ▁▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision ▁▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall ▁▂██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss ▇█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss █▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss █▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss █▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss █▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss █▄▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 █▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 ▁█▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 ▁█▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best/epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best/mAP_0.5 0.74412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best/mAP_0.5:0.95 0.48675\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/precision 0.74892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          best/recall 0.65797\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.7443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.48584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.7506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.65742\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.04378\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0.01652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.05896\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.04045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0.00877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.03652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.07778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.00078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mefficient-mountain-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/causalml/YOLOv5/runs/1x32lz2k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 113 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220913_203718-1x32lz2k/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "## Weights & Biases Logging\n",
        "\n",
        "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289).  \n",
        "\n",
        "<a href=\"https://wandb.ai/glenn-jocher/yolov5_tutorial\">\n",
        "<img alt=\"Weights & Biases dashboard\" src=\"https://user-images.githubusercontent.com/26833433/182482859-288a9622-4661-48db-99de-650d1dead5c6.jpg\" width=\"1280\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n",
        "\n",
        "This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices. \n",
        "\n",
        "<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
        "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qu7Iesl0p54"
      },
      "source": [
        "# Status\n",
        "\n",
        "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
        "\n",
        "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below for PyTorch Hub, CI, reproducing results, profiling speeds, VOC training, classification training and TensorRT example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMusP4OAxFu6"
      },
      "source": [
        "import torch\n",
        "\n",
        "# PyTorch Hub Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5n - yolov5x6, custom\n",
        "\n",
        "# Images\n",
        "img = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
        "\n",
        "# Inference\n",
        "results = model(img)\n",
        "\n",
        "# Results\n",
        "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGH0ZjkGjejy"
      },
      "source": [
        "# YOLOv5 CI\n",
        "%%shell\n",
        "rm -rf runs  # remove runs/\n",
        "m=yolov5n  # official weights\n",
        "b=runs/train/exp/weights/best  # best.pt checkpoint\n",
        "python train.py --imgsz 64 --batch 32 --weights $m.pt --cfg $m.yaml --epochs 1 --device 0  # train\n",
        "for d in 0 cpu; do  # devices\n",
        "  for w in $m $b; do  # weights\n",
        "    python val.py --imgsz 64 --batch 32 --weights $w.pt --device $d  # val\n",
        "    python detect.py --imgsz 64 --weights $w.pt --device $d  # detect\n",
        "  done\n",
        "done\n",
        "python hubconf.py --model $m  # hub\n",
        "python models/tf.py --weights $m.pt  # build TF model\n",
        "python models/yolo.py --cfg $m.yaml  # build PyTorch model\n",
        "python export.py --weights $m.pt --img 64 --include torchscript  # export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcKoSIK2WSzj"
      },
      "source": [
        "# Reproduce\n",
        "for x in (f'yolov5{x}' for x in 'nsmlx'):\n",
        "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --task speed  # speed\n",
        "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gogI-kwi3Tye"
      },
      "source": [
        "# Profile\n",
        "from utils.torch_utils import profile\n",
        "\n",
        "m1 = lambda x: x * torch.sigmoid(x)\n",
        "m2 = torch.nn.SiLU()\n",
        "results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSgFCAcMbk1R"
      },
      "source": [
        "# VOC\n",
        "for b, m in zip([64, 64, 64, 32, 16], [f'yolov5{x}' for x in 'nsmlx']):  # batch, model\n",
        "  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --img 512 --hyp hyp.VOC.yaml --project VOC --name {m} --cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification train\n",
        "for m in [*(f'yolov5{x}-cls.pt' for x in 'nsmlx'), 'resnet50.pt', 'resnet101.pt', 'efficientnet_b0.pt', 'efficientnet_b1.pt']:\n",
        "  for d in 'mnist', 'fashion-mnist', 'cifar10', 'cifar100', 'imagenette160', 'imagenette320', 'imagenette', 'imagewoof160', 'imagewoof320', 'imagewoof':\n",
        "    !python classify/train.py --model {m} --data {d} --epochs 10 --project YOLOv5-cls --name {m}-{d}"
      ],
      "metadata": {
        "id": "UWGH7H6yakVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification val\n",
        "!bash data/scripts/get_imagenet.sh --val  # download ImageNet val split (6.3G - 50000 images)\n",
        "!python classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224  # validate"
      ],
      "metadata": {
        "id": "yYgOiFNHZx-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate on COCO test. Zip results.json and submit to eval server at https://competitions.codalab.org/competitions/20794\n",
        "!bash data/scripts/get_coco.sh --test  # download COCO test-dev2017 (7G - 40000 images, test 20000)\n",
        "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half --task test"
      ],
      "metadata": {
        "id": "aq4DPWGu0Bl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTRwsvA9u7ln"
      },
      "source": [
        "# TensorRT \n",
        "!pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # install\n",
        "!python export.py --weights yolov5s.pt --include engine --imgsz 640 --device 0  # export\n",
        "!python detect.py --weights yolov5s.engine --imgsz 640 --device 0  # inference"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}